{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Latent Semantic Analysis (LSA) - Day 35\n",
    "\n",
    "Welcome to Day 35 of the **100 Days of Data Science & AI** series! Today, we explore **Latent Semantic Analysis (LSA)**, a foundational technique in Natural Language Processing (NLP) used to discover hidden (latent) themes within a collection of documents.\n",
    "\n",
    "---\n",
    "\n",
    "## üßê What is LSA?\n",
    "\n",
    "Latent Semantic Analysis (LSA) is a technique that uses **Singular Value Decomposition (SVD)** to reduce the dimensionality of a Term-Document Matrix. By doing so, it groups words that are used in similar contexts, effectively identifying \"topics\" or \"concepts.\"\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Semantic Structure**: LSA assumes that words that are close in meaning will occur in similar pieces of text.\n",
    "2. **Noise Reduction**: By keeping only the top singular values, we filter out noise (random word variations) and focus on the core semantic structure.\n",
    "3. **Topic Modeling**: It helps us answer: \"What are these documents actually about?\"\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è The Mechanics of LSA\n",
    "\n",
    "1. **Pre-processing**: Cleaning text (lowercase, removing stopwords, stemming/lemmatization).\n",
    "2. **Vectorization (TF-IDF)**: Converting text into numerical vectors where high-frequency but low-information words (like 'the') are penalized.\n",
    "3. **Truncated SVD**: Decomposing the large TF-IDF matrix into three smaller matrices: $U$, $\\Sigma$, and $V^T$. We keep only the top $k$ dimensions.\n",
    "4. **Cosine Similarity**: Comparing documents or words in this new \"latent\" space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Loading the Dataset\n",
    "\n",
    "We'll use the **20 Newsgroups** dataset, focusing on a few distinct categories to see if LSA can correctly identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['rec.sport.baseball', 'sci.space', 'talk.politics.mideast']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Total documents: {len(newsgroups.data)}\")\n",
    "print(f\"Example text:\\n{newsgroups.data[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 1: Text Vectorization (TF-IDF)\n",
    "\n",
    "We convert our text into a **TF-IDF (Term Frequency-Inverse Document Frequency)** matrix. This serves as the input for our SVD algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, max_df=0.5, min_df=2)\n",
    "X_tfidf = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {X_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß± Step 2: Singular Value Decomposition (Truncated SVD)\n",
    "\n",
    "We apply SVD to reduce our 1000-dimensional TF-IDF space into 3 topics (matching our 3 categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 3\n",
    "lsa = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "X_lsa = lsa.fit_transform(X_tfidf)\n",
    "\n",
    "print(\"Explained Variance by LSA components:\", lsa.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Visualizing the Latent Space\n",
    "\n",
    "### 1. ‚òÅÔ∏è Word Clouds for Topics\n",
    "What words define our 'latent' topics? We'll look at the top words for each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "def plot_word_clouds(lsa_model, terms, n_topics):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, comp in enumerate(lsa_model.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key=lambda x: x[1], reverse=True)[:50]\n",
    "        text = \" \".join([t[0] for t in sorted_terms])\n",
    "        cloud = WordCloud(background_color='white', width=400, height=300).generate(text)\n",
    "        \n",
    "        plt.subplot(1, n_topics, i+1)\n",
    "        plt.imshow(cloud)\n",
    "        plt.title(f\"Topic {i+1}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_word_clouds(lsa, terms, n_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. üó∫Ô∏è Document Clustering (2D Space)\n",
    "Let's visualize how the documents cluster in the new semantic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=X_lsa[:, 0], y=X_lsa[:, 1], hue=newsgroups.target, palette='Dark2', alpha=0.5)\n",
    "plt.title(\"LSA Document Clusters (Topic 1 vs Topic 2)\")\n",
    "plt.xlabel(\"Topic 1 Importance\")\n",
    "plt.ylabel(\"Topic 2 Importance\")\n",
    "plt.legend(handles=plt.gca().get_legend().legend_handles, labels=newsgroups.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. üî• Component Importance Heatmap\n",
    "Showing the top contributing terms for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms_idx = np.argsort(lsa.components_[:, :])[:, -15:]\n",
    "top_terms = [[terms[i] for i in topic] for topic in top_terms_idx]\n",
    "\n",
    "for i, t in enumerate(top_terms):\n",
    "    print(f\"Topic {i+1} Top Words: {', '.join(t[::-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "üîπ Key Takeaways\n",
    "\n",
    "‚úî **Semantic Retrieval**: LSA goes beyond keyword matching by capturing the underlying context. It can recognize that \"baseball\" and \"bat\" belong to the same topic even if they don't appear in the same document.\n",
    "\n",
    "‚úî **Efficiency**: Instead of dealing with thousands of sparse word columns, we now have a compact representation of the documents' meaning.\n",
    "\n",
    "‚úî **Topic Discover**: Using SVD, we successfully separated sports, space, and politics categories automatically from raw text.\n",
    "\n",
    "üìå Meta\n",
    "Author: Tharun Naik Ramavath\n",
    "Series: 100 Days of Data Science & AI\n",
    "Day: 35\n",
    "Platform: LinkedIn\n",
    "Notebook: Google Colab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
